<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Medical Statistics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Boncho Ku, Ph.D. in Statistics" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <script src="libs/xaringanExtra-progressBar/progress-bar.js"></script>
    <script src="libs/fabric/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="../assets/css/footer.css" type="text/css" />
    <link rel="stylesheet" href="../assets/css/metropolis-ak.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: middle, left, title-slide

# Medical Statistics
## Random Variable and Expectation
### <strong>Boncho Ku, Ph.D. in Statistics</strong>
### <br/> Korea Institute of Oriental Medicine
### <br> 2021/10/30

---






<style>.xe__progress-bar__container {
  bottom:0;
  opacity: 1;
  position:absolute;
  right:0;
  left: 0;
}
.xe__progress-bar {
  height: 0.25em;
  background-color: #23373B;
  width: calc(var(--slide-current) / var(--slide-total) * 100%);
}
.remark-visible .xe__progress-bar {
  animation: xe__progress-bar__wipe 200ms forwards;
  animation-timing-function: cubic-bezier(.86,0,.07,1);
}
@keyframes xe__progress-bar__wipe {
  0% { width: calc(var(--slide-previous) / var(--slide-total) * 100%); }
  100% { width: calc(var(--slide-current) / var(--slide-total) * 100%); }
}</style>

&lt;!-- class: inverse, center, middle --&gt;


# Random Variable `\((X)\)`

---

# Random Variable

#### Warm-Up

&gt; - Are the outcomes of rolling dices determined? `\(\rightarrow\)` **NO**
&gt; - The outcomes derived from an experiment, measurement, or survey cannot predictable `\(\rightarrow\)` **random data**


#### Preview 

&gt; Suppose you are playing a very simple game: rolling three dices
&gt; - Let `\(X\)` be the sum of three dices 
&gt;    - `\(X = 18 \rightarrow\)` win **￦50,000**
&gt;    - `\(14 \leq X \leq 17 \rightarrow\)` win **￦10,000**
&gt;    - `\(9 \leq X \leq 13 \rightarrow\)` draw
&gt;    - `\(4 \leq X \leq 8 \rightarrow\)` lose **￦12,000**
&gt;    - `\(X = 3 \rightarrow\)` lose **￦75,000**

**When you start to play this game, what things do you focus on: chance or prize?**

&gt; - We assign **prizes** to each outcome of rolling dices.
&gt; - Outcomes from rolling dices are **random** `\(\rightarrow\)` whether winning or lose the prize is **random**


---

# Random Variable 

#### **Definition**

&gt; When conducting a random experiment `\(E\)` and after learning the outcome `\(\omega\)` in `\(S\)`. Let `\(X\)` 
be a function to convert `\(\omega\)` to the real number `\(\mathbb{R}\)`, that is, 
&gt; `$$X(\omega) = x,~~ x\in \mathbb{R}$$`
&gt; A random variable `\(X\)` is a function
&gt; `$$\omega \in S \xrightarrow{X(\omega)} x \in \mathbb{R}$$`

- Usually denoted by uppercase letters such that `\(X\)`, `\(Y\)`, `\(Z\)`
- Denote observed values by lowercase letters: `\(x\)`, `\(y\)`, `\(z\)`


#### Example 1

&gt; Survey the gender of each child in a three-child household
&gt; - `\(\omega \in S = \{BBB, GBB, BGB, BBG, GGB, GBG, BGG, GGG\}\)`
&gt; - \# of Sons: `\(X(BBB)=3\)`, `\(X(GGG)=0\)`, `\(X(GBB)=2\)`


**Is it possible to define different random variables in the same `\(S\)`?**


---

# Random Variable 

#### Example 2

&gt; Let `\(E\)` be the experiment of flipping a coin twice. Then `\(S=\{HH, HT, TH, TT\}\)`. Define `\(X\)` is 
the number of heads. Then we can make a table like below: 


.panelset[
.panel[.panel-name[Table]
&lt;table class=" lightable-paper table" style='font-family: "Arial Narrow", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto; font-size: 16px; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; `\(\omega \in S\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; HH &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; HT &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; TH &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; TT &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; `\(X(\omega)=x\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

.panel[.panel-name[Scheme]
.center[
&lt;img src="assets/imgs/rv-scheme.png" width="60%"/&gt;
]
]

]


--

#### Random variables for Example 1 and 2 `\(\rightarrow\)` **discrete random variable**

&gt; - Show clear distinction between two adjacent values
&gt; - Countable/Uncountable (e.g. `\(Y\)`=number of tails before the first head)


---

# Random Variable 

#### Example 3 (**Continuous Random Variable**)

&gt; Let `\(E\)` be the experiment of tossing a coin in the air, and define the random variable `\(Z\)` is 
the time until the coin hits the ground (in second). 
&gt; - Sample space: `\(S = \{z|0 &lt; z &lt; \infty\}\)`
&gt; - Uncountable `\(\rightarrow\)` usually real numbers
&gt; - All events in `\(S\)` can be written as the combination of interval events (e.g. `\(1 \leq Z &lt; 2\)`)

#### **Probability Distribution**

&gt; - Assign probabilities coreesponding to the possible values of the random variable
&gt; `$$P(X=x) = f(x)$$`
&gt; - For Example 1, the probability distribution of `\(X\)` (\# of boys) is 

&lt;table class=" lightable-paper table" style='font-family: "Arial Narrow", arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto; font-size: 16px; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; `\(x \in S_x\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; 0 &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; 1 &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; 2 &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; 3 &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Total &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; `\(P(X=x)\)` &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1/8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3/8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3/8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1/8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Random Variable

#### **Definition of Probability Function (distribution)**

&gt; Let `\(S_X = \{x_1, x_2, \ldots, x_n\}\)` or `\(S_X = \{x_1, x_2, \ldots \}\)` and Let `\(f(x_i)\)` denote 
the probability of `\(X = x_i\)`, then the probability function (_distribution_) of `\(X\)` can be written as 
&gt; `$$f_X(x_i) = P(X = x_i), ~~~ x \in S_X$$`
&gt; - If `\(X\)` is discrete, `\(f_X(x_i)\)` refered as **_probability mass function_ (PMF)**
&gt; - If `\(X\)` is continuous, `\(f_X(x)\)` refered as **_probability density function_ (PDF)**


#### **Properties**

&gt; - `\(f_X(x) \geq 0\)` for `\(x \in S\)` 

.pull-left[
#### Discrete PMF
&gt; - `\(\sum_{x\in S}f_X(x) = 1\)`
&gt; - `\(P(a &lt; X \leq b) = \sum_{a&lt;x\leq b}f_X(x)\)`

]
.pull-right[
#### Continuous PDF
&gt; - `\(\int_{-\infty}^{\infty}f_x(x) = 1\)` 
&gt; - `\(P(a &lt; X \leq b) = \int_{a}^{b}f_X(x) dx\)`
&gt; - `\(P(X = a) = 0\)`

]

---

# Random Variable

#### **Cumulative Dsitribution Function**

&gt; The cumulative distribution function (CDF) of a random variable `\(X\)` is defined as 
&gt; `$$F_X(x) = P(X\leq x)$$`


#### **Properties**

&gt; - `\(F_X\)` is non-decreasing
&gt; - `\(F_X\)` is right-continuous 
&gt; `$$\lim_{e\rightarrow 0^+}F_X(x + e) = F_X(x),~~~~\mathrm{for~all~x\in \mathbb{R}}$$`
&gt; - `\(\lim_{x\rightarrow -\infty}F_X(x) = 0\)` and `\(\lim_{x\rightarrow \infty}F_X(x) = 1\)`


- Discrete `\(X\)`: step function
- Continuous `\(X\)`: monotonic increasing function (continuous)

---

# Random Vairable 

#### **Cumulative Dsitribution Function**

&gt; If `\(X\)` is discrete, the cdf of `\(X\)` is 
&gt; `$$F_X(x) = P(X\leq x) = \sum_{\{k\leq x; f_X(k)&gt;0\}}P_X(k)$$`
&gt; If `\(X\)` is continuous
&gt; `$$F_X(x) = P(X\leq x) = \int_{-\infty}^{x}f_X(t) dt$$`


We say that `\(X\)` has the distribution `\(F_X\)`, write `\(X\sim F_X\)`. Similarly, `\(X \sim f_X\)` for the 
named distributions of pmf of pdf 

---

# Expectation

#### Concept 

&gt; 1. **If the structure of the random variable is revealed, which means that the probability function of 
the random variable is known, what is the theoretical average of the random variable?**
&gt; 2. Suppose we could observe how the weather is in Christmas every year repeatedly. Suppose that
the weather could be snowing (1) or not snowing (0). Each time the observation is repeated,
we get an outcome for the random variable. We end up with `\(n\)` outcomes. The average of the random
variable is just summing up all outcomes and dividing it by `\(n\)`. Now imagine that `\(n\)` goes to
larger and larger without bound?



---

# Expectation


#### **Definition** 

&gt; Let the pmf of the random variable `\(X\)` be `\(f_X(x)\)`. The expectation of the discrete random variable `\(X\)` is 
&gt; `$$\mu_X = E(X) = \sum_{x_i \in S}x_i f_X(x)$$`
&gt; **Weighted Average of `\(x_i\)`** `\(\rightarrow\)` weights = `\(f_X(x_i)\)`


#### Example 4: Expectation of Example 1

`$$\mu_X = \sum_{x=0}^{3}xf_X(x)=0\cdot\frac{1}{8} + 1\cdot\frac{3}{8} + 2\cdot\frac{3}{8} + 3\cdot\frac{1}{8} = 1.5$$`

#### Remarks

&gt; Although we say `\(X\)` is 1.5 on the average, the average of `\(X\)` in real world never actually equals to 1.5. 


???

We interpret μ = 1.5 by reasoning that if we were to repeat the random experiment many times,
independently each time, observe many corresponding outcomes of the random variable X, and
take the sample mean of the observations, then the calculated value would fall close to 1.5. The
approximation would get better as we observe more and more values of X (another form of the
Law of Large Numbers; see Section 4.3). Another way it is commonly stated is that X is 1.5
“on the average” or “in the long run”.


---

# Expectation

#### Recall the formula to calculate a sample mean 

`$$\bar{x} = \frac{1}{N}\sum_{i=1}^{N}x_i$$`

&gt; - The meaning of `\(x_i\)` is different from the definition of the expectation
&gt; - In the expectation, `\(x_i\)` represents all possible elements from the sample space
&gt; - In the sample mean, `\(x_i\)` is the realized samples


#### Question 

&gt; In the formula for calculating the expectation, the probability roles as a weight. 
But why is there no probability weight in the formula for calculating the sample mean?


---

# Expectation

#### **General Definition**

&gt; Let the pmf (or pdf) of the random variable `\(X\)` be `\(f_X(x)\)`. The expectation of `\(g(X)\)` is 
`$$\begin{aligned}
E[g(X)] &amp;= \sum_{x:f_X(x)&gt;0} g(x)f_X(x)~~~\mathrm{for~pmf} \\
        &amp;= \int_{-\infty}^{\infty} g(x)f_X(x) dx~~~\mathrm{for~pdf}
\end{aligned}$$`


#### **Properties**

&gt; For any functions `\(g\)` and `\(h\)`, any random variable `\(X\)`, and any constant `\(c\)`: 
&gt; - `\(E(c) = c\)`
&gt; - `\(E[cg(X)] = cE[g(X)]\)`
&gt; - `\(E[g(X) + h(X)] = E[g(X)] + E[h(X)]\)`


---

# Expectation

#### **Relationship betwee n the expectation and sample mean**

&lt;!-- &gt; Question 1: Is a sample mean random variable?  --&gt;
&gt; - Suppose we have a random sample size of `\(N\)`, denoted as `\(\{X_1, X_2, \ldots, X_N\}\)`. Then 
sample mean of the random variable `\(X\)` is 
&gt; `$$\bar{X} = \frac{1}{N}\sum_{i=1}^{N}X_i$$`
&gt; - Since `\(\{X_1, \ldots, X_N\}\)` are randomly selected observations and each `\(X_i\)` has the 
distribution, `\(\bar{X}\)` is an random variable. 

`$$\begin{aligned}
E(\bar{X}) &amp;= E\left(\frac{1}{N}\sum_{i=1}^{N}X_i\right) \\
           &amp;= \frac{1}{N}\sum_{i=1}^{N}E(X_i) = \frac{1}{N}N E(X) \\
           &amp;= E(X) = \mu_X
\end{aligned}$$`


---

# Variance 

#### **Variance of Random Variable**

&gt; - The variance of a random variable indicates the shape of the pmf or pdf: whether the probability is 
dense or spread
&gt; - Roughly speaking, the expectation is a predicted value from the random variable, while the variance 
indicates the precision or reliability of the prediction.


#### **Definition**

&gt; - Let the pmf (or pdf) of a random variable `\(X\)` be `\(f_X(x)\)`. The operator of the variance for a 
random variable `\(X\)` is denoted as `\(\mathrm{Var}(X)\)` and its result is written as `\(\sigma^2\)`

`$$\begin{aligned}
\sigma^2 = \mathrm{Var}(X) &amp;= E[(X - \mu)^2] \\
                           &amp;= \sum_{x \in S}(x_i - \mu)^2f_X(x_i),~~~\mathrm{discrete} \\
                           &amp;= \int_{-\infty}^{\infty} (x - \mu)^2f_X(x) dx,~~~\mathrm{continuous}
\end{aligned}$$`


---
# Variance

#### **Properties**

&gt; For any random variable `\(X\)` and any constant `\(c\)`
&gt; - `\(\mathrm{Var}(X) \geq 0\)`
&gt; - `\(\mathrm{Var}(c) = 0\)`, `\(\mathrm{Var}(cX) = c^2\mathrm{Var}(X)\)`
&gt; - `\(\mathrm{Var(X + c)} = \mathrm{Var(X)}\)`
&gt; - `\(\mathrm{Var}(X) = E[X^2] - (E[X])^2 = E[X^2] - \mu_X^{2}\)`


#### The variance of sum (or difference) of two random variables

&gt; Let `\(X\)` and `\(Y\)` be random variables with the probability distribution, then 

`$$\begin{aligned}
\mathrm{Var}(X\pm Y) &amp;= E[(X \pm Y - (\mu_X \pm \mu_Y))^2] \\
                     &amp;= E[((X - \mu_X) \pm (Y - \mu_Y))^2] \\ 
                     &amp;= E[(X - \mu_X)^2 + (Y - \mu_Y)^2 \pm 2(X-\mu_X)(Y - \mu_Y)] \\
                     &amp;= E[(X - \mu_X)^2] + E[(Y - \mu_Y)^2] \pm 2E[(X-\mu_X)(Y - \mu_Y)] \\
                     &amp;= \mathrm{Var}(X) + \mathrm{Var}(Y) \pm 2\mathrm{Cov}(X, Y)
\end{aligned}$$`

---
# Variance 

#### <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> Example

&gt; Suppose `\(X \sim N(0, 1)\)` and `\(Y \sim N(2, 4)\)`
&gt; - Generate 100 random samples corresponding to `\(X\)` and `\(Y\)` 
&gt; - Let `\(Z = X + Y\)`
&gt; - Calcuate sample means and variances `\(X\)`, `\(Y\)` and `\(Z\)`

.panelset[
.panel[.panel-name[R Code]

```r
set.seed(123)
X &lt;- rnorm(100, 0, 1)
Y &lt;- rnorm(100, 2, 2)
Z &lt;- X+Y
mX &lt;- mean(X); mY &lt;- mean(Y)
mZ &lt;- mean(Z)
vX &lt;- var(X); vY &lt;- var(Y)
vZ &lt;- var(Z)
```
]

.panel[.panel-name[Results]

```r
mX + mY; mZ
```

```
[1] 1.875312
```

```
[1] 1.875312
```

```r
vX + vY; vZ
```

```
[1] 4.573485
```

```
[1] 4.398601
```
]

]

---
# Variance 

#### **Variance of the Sample Mean**

&gt; Let `\(X_1, \ldots, X_n\)` are random samples from the identical and independent distribution, then 
&gt; `$$\mathrm{Var}(\bar{X}) = \frac{1}{N}\mathrm{Var}(X) = \frac{\sigma^2}{N}$$`

#### **Proof**

`$$\begin{aligned}
\mathrm{Var}(\bar{X}) &amp;= \mathrm{Var}\left(\frac{1}{N}\sum_{i=1}^{N}X_i \right) = \frac{1}{N^2}\mathrm{Var(X_1 + \cdots + X_N)} \\ 
                      &amp;= \frac{1} {N^2}N\mathrm{Var}(X) = \frac{1}{N}\mathrm{Var}(X) = \frac{\sigma^2}{N}
\end{aligned}$$`

---
# Variance 

#### **Expectation of the Sample Variance** 
&gt; Let `\(\bar{X}\)`, `\(\sigma^2_X\)`, and `\(S^2\)` be the sample mean, variance, and sample variance of a random variable `\(X\)`. 
Then the expectation of `\(S^2\)` is 
&gt; `$$E(S^2) = \frac{N-1}{N}\sigma^2_X,~~~S^2 = \frac{1}{N}\sum_{i=1}^{N}(X_i - \bar{X})^2$$`


.panelset[
.panel[.panel-name[Proof: Part I]

`$$\begin{aligned}
E(S^2) &amp;= E\left[\frac{1}{N}\sum_{i=1}^{N}(X_i - \bar{X})^2 \right] = E\left[\frac{1}{N}\sum_{i=1}^{N}(X^2_i - 2X_i\bar{X} + \bar{X}^2) \right] \\ 
       &amp;= E\left[\frac{1}{N}\sum_{i=1}^{N}X_i^2 -2\bar{X}\frac{1}{N}\sum_{i=1}^{N}X_i + \frac{1}{n}\sum_{i=1}^{N}\bar{X}^2  \right] \\ 
       &amp;= \frac{1}{N}\sum_{i=1}^{N}E(X_i^2) - E(\bar{X}^2)
\end{aligned}$$`

]

.panel[.panel-name[Part II]
Since `\(\sigma^2_X = E(X^2) - \mu_X^2\)` and `\(\mathrm{Var}(\bar{X}) = E[\bar{X}^2] - \mu_X^2\)`

`$$\begin{aligned}
E(X^2) &amp;= \sigma^2_X + \mu_X^2 \\
E[\bar{X}^2] &amp;= \frac{\sigma^2_X}{N} + \mu_X^2 
\end{aligned}$$`
]

.panel[.panel-name[Part III]
Using Part II, 

`$$\begin{aligned}
E(S^2) &amp;= \frac{1}{N}\sum_{i=1}^{N}E(X_i^2) - E(\bar{X}^2) = \frac{1}{N}\sum_{i=1}^{N}(\sigma_X^2 + \mu_X^2) - E[\bar{X}^2] \\
       &amp;= \frac{1}{N}N(\sigma^2_X + \mu_X^2) - \left(\frac{\sigma^2_X}{N} + \mu_X^2\right) \\ 
       &amp; = \frac{N-1}{N}\sigma_X^2
\end{aligned}$$`

]

.panel[.panel-name[Implication]

&gt; - Unlike the sample mean `\(\bar{X}\)`, the expectation of the sample variance `\(S^2\)` is less than `\(\sigma_X^2\)` (**biased**)
&gt; - To obtain the unbiased result, therefore, use `\(N-1\)` 

]
]


---
# Expectation and Variance 

#### **Implication**

&gt; 1. The p.m.f or p.d.f of `\(X\)` shall exist to derive the expectation (or variance) but we don't know 
the exact formula of p.m.f or p.d.f. 
&gt; 2. Since `\(E(\bar{X}) = E(X)\)`, the sample mean `\(\bar{x}\)` is approximate to `\(E(X)\)`
&gt; 3. If `\(N\)` is very large, `\(\bar{x}\)` is nearly or equal to `\(E(X)\)` since the variance of `\(\bar{x}\)` becomes small. 
&gt; 4. Therefore, `\(\bar{x} \approx E(X)\)` when `\(N\)` is very large


???
`$$\begin{aligned}
\sigma^2_X &amp;= \frac{N}{N-1}E(S^2) \\
           &amp;= \frac{N}{N-1}E\left[\frac{1}{N}\sum_{i=1}^{N}(X_i - \bar{X})^2\right] \\
           &amp;= E\left[\frac{1}{N-1}\sum_{i=1}^{N}(X_i - \bar{X})^2\right]
\end{aligned}$$`


---

# Joint and Marginal Distribution 

#### **Joint and Marginal Distribution for Two Categorical Vairables**

&gt; Consider two discrete random variables `\(X\)` and `\(Y\)` with p.m.f `\(f_X\)` and `\(f_Y\)` supporting on the 
sample space `\(S_X\)` and `\(S_Y\)`, respectively. Let `\(S_{X,Y}\)` denote the set of all possible pairs 
`\((x, y)\)`. Then the _**joint probability mass function**_ of `\(X\)` and `\(Y\)` is the function `\(f_{X,Y}\)` is

`$$f_{X,Y} = P(X = x, Y = y)~~~~\mathrm{for~(x,y) \in S_{X,Y}}$$`

&gt; For every joint p.m.f 

`$$\begin{aligned}
f_{X, Y}(x, y) &amp;&gt; 0~~~~\mathrm{for~all~(x,y) \in S_{X,Y}} \\
\sum_{(x,y)\in S_{X,Y}} f_{X,Y}(x, y) &amp;= \sum_{x\in S_X}\sum_{y \in S_Y} f_{X,Y}(x, y) = 1
\end{aligned}$$`


---
# Joint and Marginal Distribution 

#### **Marginal Distribution**

&gt; The p.m.f's of `\(f_(X)\)` and `\(f_(Y)\)` are called the _marginal p.m.f of `\(X\)` and `\(Y\)`_, respectively, 
in the context of the joint probability distribution

`$$\begin{aligned}
f_X(x) &amp;= P(X = x) \\
       &amp;= \sum_{y \in S_Y} P(X = x, Y = y) \\
       &amp;= \sum_{y \in S_Y} f_{X,Y}(x, y)
\end{aligned}$$`

&gt; Similarly, 
&gt; `$$f_Y(y) = \sum_{x\in S_X} f_{X,Y}(x,y)$$`


#### **Joint Cumulative Distribution Function**

`$$F_{X,Y}(x,y)=P(X\leq x, Y \leq y)~~~~\mathrm{for~all~(x,y)}\in \mathbb{R}^2$$`

???

Given the joint PMF we may recover the marginal PMFs, but the converse is not true. Even if
we have both marginal distributions they are not sufficient to determine the joint PMF; more
information is needed


**Cumulative probability mass function**

`$$P(a\leq X\leq b, c \leq Y \leq d) = \sum_{a\leq X \leq b}\sum_{c\leq X \leq d} f_{X,Y}(x, y)$$`
---
# Joint and Marginal Distribution 

#### **Example**

&gt; The 50 students at a university take examinations in two subjects `\(X\)` and `\(Y\)`, and results 
come out with A, B, C, D, E, F grades. 

.panelset[
.panel[.panel-name[R Code]

```r
library(tidyverse)

grades &lt;- LETTERS[1:6]
dat &lt;- expand_grid(X = grades, Y = grades)
dat &lt;- dat %&gt;% 
  mutate(n = c(1,2,1,0,0,0, 
               0,2,3,1,0,0, 
               0,4,7,4,1,0, 
               0,1,4,5,4,0,
               0,0,1,3,2,0, 
               0,0,0,1,2,1))
```

]
.panel[.panel-name[Table &amp; Joint p.m.f]
.pull-left[

```r
library(questionr)
t1 &lt;- with(dat, wtd.table(X, Y, 
           weights = n)); t1
```

```
  A B C D E F
A 1 2 1 0 0 0
B 0 2 3 1 0 0
C 0 4 7 4 1 0
D 0 1 4 5 4 0
E 0 0 1 3 2 0
F 0 0 0 1 2 1
```
]
.pull-right[

```r
# Joint probability mass function
t1/sum(t1)
```

```
     A    B    C    D    E    F
A 0.02 0.04 0.02 0.00 0.00 0.00
B 0.00 0.04 0.06 0.02 0.00 0.00
C 0.00 0.08 0.14 0.08 0.02 0.00
D 0.00 0.02 0.08 0.10 0.08 0.00
E 0.00 0.00 0.02 0.06 0.04 0.00
F 0.00 0.00 0.00 0.02 0.04 0.02
```
]
]

.panel[.panel-name[Heatmap-Code]

```r
library(ggpubr)
gp1 &lt;- ggplot(data = dat) + 
  aes(x = X, y = Y, fill = n/60, label = sprintf("%.2f", n/60)) + 
  geom_tile(color = "white") + 
  scale_fill_gradient(
    low = "white", high = "#132B43", limits = c(0, 0.15), 
    name = "", breaks = c(0, 0.05, 0.10, 0.15)
  ) +
  labs(title = "Joint Probabilty Mass Function of X and Y") + 
  geom_text(color = "black", size = 4) +
  guides(fill = guide_colorbar(barheight = 25)) + 
  scale_y_discrete(limits = rev) + 
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(), axis.title = element_blank())
```

]

.panel[.panel-name[Plot]
&lt;img src="Figures/pmf-ex-1.png" width="380" height="380" style="display: block; margin: auto;" /&gt;

]

.panel[.panel-name[Marginal Distribution]
.pull-left[

```r
# Marginal pmf of X
dat %&gt;% 
  count(X, wt = n) %&gt;% 
  mutate(n = n/sum(n))
```

```
# A tibble: 6 × 2
  X         n
  &lt;chr&gt; &lt;dbl&gt;
1 A      0.08
2 B      0.12
3 C      0.32
4 D      0.28
5 E      0.12
6 F      0.08
```

]
.pull-right[

```r
# Marginal pmf of YY
dat %&gt;% 
  count(Y, wt = n) %&gt;% 
  mutate(n = n/sum(n))
```

```
# A tibble: 6 × 2
  Y         n
  &lt;chr&gt; &lt;dbl&gt;
1 A      0.02
2 B      0.18
3 C      0.32
4 D      0.28
5 E      0.18
6 F      0.02
```

]
]

]


---
# Joint and Marginal Distribution

#### **Joint and Marginal Distribution for Continuous Variables**

&gt; Given two continuous random variable `\(X\)` and `\(Y\)`, let `\(f_{X,Y}(x, y)\)` be a function associated 
with `\(X = x\)` and `\(Y = y\)`. Then `\(f\)` is called the _**joint probability density function**_ of 
`\(X\)` and `\(Y\)` if 

`$$\begin{aligned}
f_{X,Y}(x,y) &amp;\geq 0~~~~\mathrm{for~all}~(x,y)\in S_{X,Y} \\
\int_{S_X}\int_{S_Y} f_{X,Y}(x,y)dx dy &amp;= 1
\end{aligned}$$`


&gt; Joint Cumulative Distribution Function 

`$$F_{X,Y}(x, y) = P(X \leq x, Y\leq y) = \int_{-\infty}^{x}\int_{-\infty}^{y} f_{X,Y}(u, v) dv du
~~~~\mathrm{for~all}~(x,y) \in \mathbb{R}^2$$`

&gt; Marginal PDF of `\(X\)` and `\(Y\)`

`$$\begin{aligned}
f_X(x) &amp;= \int_{S_Y} f_{X,Y}(x,y) dy,~~~~x \in S_X \\
f_Y(y) &amp;= \int_{S_X} f_{X,Y}(x,y) dx,~~~~y \in S_Y \\
\end{aligned}$$`



???
(**For Joint CDF**) In the continuous case there is not such a simple interpretation for the joint 
PDF; however,we do have one for the joint CDF, namely


---
# Joint and Marginal Distribution

#### **Example**: Joint PDF

&gt; Let the joint PDF of `\((X,Y)\)` be given by 
&gt; `$$f_{X,Y}(x,y) = \frac{6}{5}(x + y^2), ~~~~0&lt;x&lt;1,~0&lt;y&lt;1$$`

.panelset[
.panel[.panel-name[Marginal PDF of X]
`$$\begin{aligned}
f_X(x) &amp;= \int_0^1 \frac{6}{5}(x + y^2) dy \\
       &amp;= \frac{6}{5}\left(xy + \frac{y^3}{3}\right) \bigg|_{y=0}^{1} \\
       &amp;= \frac{6}{5}\left(x + \frac{1}{3}\right)
\end{aligned}$$`
]
.panel[.panel-name[Marginal PDF of Y]
`$$\begin{aligned}
f_X(y) &amp;= \int_0^1 \frac{6}{5}(x + y^2) dx \\
       &amp;= \frac{6}{5}\left(\frac{x^2}{2} + xy^2\right) \bigg|_{x=0}^{1} \\
       &amp;= \frac{6}{5}\left(\frac{1}{2} + y^2\right)
\end{aligned}$$`
]
.panel[.panel-name[Probabilty of joint PDF]
`$$\begin{aligned}
P(0&lt;x&lt;0.3,~0.1&lt;y&lt;0.3) &amp;= \int_{x=0}^{0.3}\int_{y=0.1}^{0.3}\frac{6}{5}\left(x + y^2\right) dy dx \\
                      &amp;= \int_{x=0}^{0.3}\frac{6}{5}\left(xy + \frac{y^3}{3}\right)\bigg|_{y=0.1}^{0.3} dx \\
                      &amp;= \int_{x=0}^{0.3}\frac{6}{5}\left(0.2x + 0.00867\right) dx \\
                      &amp;= \frac{6}{5}\left(\frac{0.2}{2}x^2 + 0.00867x\right)\bigg |_{x=0}^{0.3} \approx 0.014
\end{aligned}$$`
]

]

---
# Joint and Marginal Expectation

&gt; Given a function `\(g\)` with arguments `\((x, y)\)`, The expectation of `\(g(x, y)\)` wit respect to the 
joint PDF (or PMF) is

`$$\begin{aligned}
E\left[g(X,Y)\right] &amp;= \int_{S_X}\int_{S_Y}g(x,y)f_{X,Y}(x,y) dx dy,~~~~\mathrm{for~the~continuous~case} \\
E\left[g(X,Y)\right] &amp;= \sum_{x\in S_X}\sum_{y \in S_Y}g(x,y)f_{X,Y}(x,y),~~~~\mathrm{for~the~discrete~case}
\end{aligned}$$`

#### **Example**

&gt; From the previous example for the joint PDF of `\(X\)` and `\(Y\)`, 

`$$\begin{aligned}
E[X]  &amp;= \int_{x=0}^{1}\int_{y=0}^{1} x\frac{6}{5}(x + y^2) dy dx = 0.6 \\
E[XY] &amp;= \int_{x=0}^{1}\int_{y=0}^{1} xy\frac{6}{5}(x + y^2) dy dx = 0.35 \\
E[X^2] &amp;= \int_{x=0}^{1}\int_{y=0}^{1} x^2\frac{6}{5}(x + y^2) dy dx = 0.43
\end{aligned}$$`

---
# Joint and Marginal Expectation

#### **Covariance and Correlation**

&gt; Two special cases of joint expectations `\(\rightarrow\)` quantifying the dependence (relationship) between `\(X\)` and `\(Y\)`

#### **Definition: Covariance**

&gt; The _**covariance**_ of `\(X\)` and `\(Y\)` is defined as 

`$$\begin{aligned}
\mathrm{Cov}(X,Y) = \sigma_{XY}&amp;= E[X - E(X)]E[Y - E(Y)] \\
                  &amp;= E[XY] - E[X]E[Y]
\end{aligned}$$`


#### **Definition: Correlation**

&gt; The Pearson product moment correlation between `\(X\)` and `\(Y\)` is the covariance between `\(X\)` and
`\(Y\)` rescaled to fall in the interval [−1, 1]


`$$\begin{aligned}
\mathrm{Corr}(X,Y) = \rho_{XY} &amp;= E\left[\left(\frac{X - \mu_X}{\sigma_X}\right)\left(\frac{X-\mu_Y}{\sigma_Y}\right)\right] \\
                               &amp;= \frac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}
\end{aligned}$$`

---
# Joint and Marginal Expectation

#### **Homework for Practice**

&gt; Let the joint PMF of `\(X\)` and `\(Y\)` be given as

&lt;table class=" lightable-paper" style='font-family: "Arial Narrow", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Y=0 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Y=1 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Y=2 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Y=3 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; X=0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.05 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.05 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; X=1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.05 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.25 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.10 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; X=2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.15 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.05 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&gt; - Define the marginal PMF of `\(X\)` and `\(Y\)`
&gt; - Obtain `\(E(X)\)`, `\(E(Y)\)`, `\(Var(X)\)`, `\(Var(Y)\)` for the marginal distribution of `\(X\)` and `\(Y\)`
&gt; - Obtain `\(E(XY)\)`
&gt; - Obtain `\(\mathrm{Cov}(X,Y)\)`
&gt; - Obtain `\(\mathrm{Corr}(X,Y)\)`





    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
