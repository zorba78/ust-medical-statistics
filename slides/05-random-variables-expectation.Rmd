---
title: "Medical Statistics"
subtitle: "Random Variable and Expectation"
author: "<strong>Boncho Ku, Ph.D. in Statistics</strong>"
institute: "<br/> Korea Institute of Oriental Medicine"
date: "<br> 2023/10/19 "
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, ../assets/css/footer.css, ../assets/css/metropolis-ak.css]
    # css: [default, metropolis, metropolis-fonts]
    # lib_dir: "../docs/libs"
    lib_dir: "libs"
    nature:
      highlightStyle: github
      titleSlideClass: [middle, left]
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---



```{r setup-knitr, include=FALSE}
options(htmltools.dir.version = FALSE)
# knitr::opts_knit$set(root.dir='..')
knitr::opts_chunk$set(eval = TRUE, 
                      echo = FALSE, 
                      cache = FALSE,
                      include = TRUE,
                      collapse = FALSE,
                      message=FALSE,
                      warning=FALSE, 
                      dependson = NULL,
                      engine = "R", # Chunks will always have R code, unless noted
                      error = TRUE,
                      fig.path="Figures/",  # Set the figure options
                      fig.align = "center", 
                      #fig.width = 7,
                      #fig.height = 7, 
                      fig.keep='all', fig.retina=3)

```

```{r setup-library}
library(MASS)
library(reshape2)
library(plyr)
library(tidyverse)
library(lubridate)
library(readxl)
library(tidyselect)
library(tidystats)
library(glue)
library(here)
library(gt)
library(gtsummary)
library(kableExtra)
library(icons)

xaringanExtra::use_tile_view()
xaringanExtra::use_progress_bar(color = "#23373B", location = "bottom")
xaringanExtra::use_scribble()
xaringanExtra::use_panelset()
# xaringanExtra::style_panelset_tabs(foreground = "honeydew", background = "seagreen")

```

<!-- class: inverse, center, middle -->


# Random Variable $(X)$

---

# Random Variable

#### Warm-Up

> - Are the outcomes of rolling dices determined? $\rightarrow$ **NO**
> - The outcomes derived from an experiment, measurement, or survey cannot predictable $\rightarrow$ **random data**


#### Preview 

> Suppose you are playing a very simple game: rolling three dices
> - Let $X$ be the sum of three dices 
>    - $X = 18 \rightarrow$ win **￦50,000**
>    - $14 \leq X \leq 17 \rightarrow$ win **￦10,000**
>    - $9 \leq X \leq 13 \rightarrow$ draw
>    - $4 \leq X \leq 8 \rightarrow$ lose **￦12,000**
>    - $X = 3 \rightarrow$ lose **￦75,000**

**When you start to play this game, what things do you focus on: chance or prize?**

> - We assign **prizes** to each outcome of rolling dices.
> - Outcomes from rolling dices are **random** $\rightarrow$ whether winning or lose the prize is **random**


???

**Intro**

Let's assume you are a doctor. You receive various data from a patient. The data that a patient provides can be broadly categorized into two types (not in terms of variable type like quantitative or qualitative data): deterministic data, such as date of birth, which you only need to ask once and won't change, and stochastic data, like blood pressure and body temperature, which you need to ask every time the patient comes in. The reason you only need to ask for a patient's date of birth once is that no matter who asks or when they ask, the value won't change. In contrast, stochastic data, such as blood pressure, can yield different values each time. Even if you've measured it 100 times before, you can't predict with 100% accuracy what value will come up next. Moreover, even when measuring the same person's blood pressure at the same time, the result can vary depending on the person's conditions and the equipment. However, saying that you can't predict blood pressure with 100% accuracy doesn't mean you have no information or knowledge about blood pressure data. For example, if a blood pressure monitor shows a value of 1000 or -10, we would assume the blood pressure monitor is malfunctioning.


---

# Random Variable 

#### **Definition**

> When conducting a random experiment $E$ and after learning the outcome $\omega$ in $S$. Let $X$ 
be a function to convert $\omega$ to the real number $\mathbb{R}$, that is, 
> $$X(\omega) = x,~~ x\in \mathbb{R}$$
> A random variable $X$ is a function
> $$\omega \in S \xrightarrow{X(\omega)} x \in \mathbb{R}$$

- Usually denoted by uppercase letters such that $X$, $Y$, $Z$
- Denote observed values by lowercase letters: $x$, $y$, $z$


#### Example 1

> Survey the gender of each child in a three-child household
> - $\omega \in S = \{BBB, GBB, BGB, BBG, GGB, GBG, BGG, GGG\}$
> - \# of Sons: $X(BBB)=3$, $X(GGG)=0$, $X(GBB)=2$


**Is it possible to define different random variables in the same $S$?**


---

# Random Variable 

#### Example 2

> Let $E$ be the experiment of flipping a coin twice. Then $S=\{HH, HT, TH, TT\}$. Define $X$ is 
the number of heads. Then we can make a table like below: 


.panelset[
.panel[.panel-name[Table]
```{r}
x <- c("$X(\\omega)=x$", "2", "1", "1", "0")
nx <- c("$\\omega \\in S$", "HH", "HT", "TH", "TT")
tab <- matrix(x, nrow = 1) %>% as.data.frame %>% as_tibble
names(tab) <- nx
kbl(tab, escape = FALSE) %>%
  kable_paper %>% 
  kable_styling(font_size = 16)
  
# kbl(t1, booktabs = TRUE, caption = "Contingency Table") %>% kable_paper %>% kable_styling(font_size = 14)

```
]

.panel[.panel-name[Scheme]
.center[
<img src="assets/imgs/rv-scheme.png" width="60%"/>
]
]

]


--

#### Random variables for Example 1 and 2 $\rightarrow$ **discrete random variable**

> - Show clear distinction between two adjacent values
> - Countable/Uncountable (e.g. $Y$=number of tails before the first head)


---

# Random Variable 

#### Example 3 (**Continuous Random Variable**)

> Let $E$ be the experiment of tossing a coin in the air, and define the random variable $Z$ is 
the time until the coin hits the ground (in second). 
> - Sample space: $S = \{z|0 < z < \infty\}$
> - Uncountable $\rightarrow$ usually real numbers
> - All events in $S$ can be written as the combination of interval events (e.g. $1 \leq Z < 2$)

#### **Probability Distribution**

> - Assign probabilities coreesponding to the possible values of the random variable
> $$P(X=x) = f(x)$$
> - For Example 1, the probability distribution of $X$ (\# of boys) is 

```{r}
x <- c("$P(X=x)$", "1/8", "3/8", "3/8", "1/8", "1")
nx <- c("$x \\in S_x$", "0", "1", "2", "3", "Total")
tab <- matrix(x, nrow = 1) %>% as.data.frame %>% as_tibble
names(tab) <- nx
kbl(tab, escape = FALSE) %>%
  kable_paper(full_width = FALSE) %>% 
  kable_styling(font_size = 16)

```  

---

# Random Variable

#### **Definition of Probability Function (distribution)**

> Let $S_X = \{x_1, x_2, \ldots, x_n\}$ or $S_X = \{x_1, x_2, \ldots \}$ and Let $f(x_i)$ denote 
the probability of $X = x_i$, then the probability function (_distribution_) of $X$ can be written as 
> $$f_X(x_i) = P(X = x_i), ~~~ x \in S_X$$
> - If $X$ is discrete, $f_X(x_i)$ refered as **_probability mass function_ (PMF)**
> - If $X$ is continuous, $f_X(x)$ refered as **_probability density function_ (PDF)**


#### **Properties**

> - $f_X(x) \geq 0$ for $x \in S$ 

.pull-left[
#### Discrete PMF
> - $\sum_{x\in S}f_X(x) = 1$
> - $P(a < X \leq b) = \sum_{a<x\leq b}f_X(x)$

]
.pull-right[
#### Continuous PDF
> - $\int_{-\infty}^{\infty}f_x(x) = 1$ 
> - $P(a < X \leq b) = \int_{a}^{b}f_X(x) dx$
> - $P(X = a) = 0$

]

---

# Random Variable

#### **Cumulative Dsitribution Function**

> The cumulative distribution function (CDF) of a random variable $X$ is defined as 
> $$F_X(x) = P(X\leq x)$$


#### **Properties**

> - $F_X$ is non-decreasing
> - $F_X$ is right-continuous 
> $$\lim_{e\rightarrow 0^+}F_X(x + e) = F_X(x),~~~~\mathrm{for~all~x\in \mathbb{R}}$$
> - $\lim_{x\rightarrow -\infty}F_X(x) = 0$ and $\lim_{x\rightarrow \infty}F_X(x) = 1$


- Discrete $X$: step function
- Continuous $X$: monotonic increasing function (continuous)

---

# Random Vairable 

#### **Cumulative Dsitribution Function**

> If $X$ is discrete, the cdf of $X$ is 
> $$F_X(x) = P(X\leq x) = \sum_{\{k\leq x; f_X(k)>0\}}P_X(k)$$
> If $X$ is continuous
> $$F_X(x) = P(X\leq x) = \int_{-\infty}^{x}f_X(t) dt$$


We say that $X$ has the distribution $F_X$, write $X\sim F_X$. Similarly, $X \sim f_X$ for the 
named distributions of pmf of pdf 


???

**End of Slide**

When performing data analysis, variables you are interesting can be regarded as random variables. Analyzing data under the framework of random variable, the analysis typically follows these steps: 

Collect data.
Assume that the collected data represents a sample of a probability variable.
Use the data to determine the shape of the probability distribution function of that probability variable.
Predict the next data points or data characteristics from the determined probability variable.
Among these steps, the most crucial one is the third step, where you reverse-engineer the probability distribution function from the data. There are various methods to determine the shape of the probability distribution function from data, and one of the simplest ways is to use descriptive statistics as follows:

Calculate descriptive statistics values like sample mean and sample variance from the data distribution.
Find a probability distribution function that has the same descriptive statistics values.
To use this approach, you need to know how to calculate descriptive statistics values for a probability distribution without sample data. In the following section, we will study the descriptive statistics values of probability distribution functions, such as the mean and variance.



---

# Expectation

#### Concept 

> 1. **If the structure of the random variable is revealed, which means that the probability function of 
the random variable is known, what is the theoretical average of the random variable?**
> 2. Suppose we could observe how the weather is in Christmas every year repeatedly. Suppose that
the weather could be snowing (1) or not snowing (0). Each time the observation is repeated,
we get an outcome for the random variable. We end up with $n$ outcomes. The average of the random
variable is just summing up all outcomes and dividing it by $n$. Now imagine that $n$ goes to
larger and larger without bound?



---

# Expectation


#### **Definition** 

> Let the pmf of the random variable $X$ be $f_X(x)$. The expectation of the discrete random variable $X$ is 
> $$\mu_X = E(X) = \sum_{x_i \in S}x_i f_X(x)$$
> **Weighted Average of $x_i$** $\rightarrow$ weights = $f_X(x_i)$


#### Example 4: Expectation of Example 1

$$\mu_X = \sum_{x=0}^{3}xf_X(x)=0\cdot\frac{1}{8} + 1\cdot\frac{3}{8} + 2\cdot\frac{3}{8} + 3\cdot\frac{1}{8} = 1.5$$

#### Remarks

> Although we say $X$ is 1.5 on the average, the average of $X$ in real world never actually equals to 1.5. 


???

We interpret μ = 1.5 by reasoning that if we were to repeat the random experiment many times,
independently each time, observe many corresponding outcomes of the random variable X, and
take the sample mean of the observations, then the calculated value would fall close to 1.5. The
approximation would get better as we observe more and more values of X (another form of the
Law of Large Numbers; see Section 4.3). Another way it is commonly stated is that X is 1.5
“on the average” or “in the long run”.


---

# Expectation

#### Recall the formula to calculate a sample mean 

$$\bar{x} = \frac{1}{N}\sum_{i=1}^{N}x_i$$

> - The meaning of $x_i$ is different from the definition of the expectation
> - In the expectation, $x_i$ represents all possible elements from the sample space
> - In the sample mean, $x_i$ is the realized samples


#### Question 

> In the formula for calculating the expectation, the probability roles as a weight. 
But why is there no probability weight in the formula for calculating the sample mean?


---

# Expectation

#### **General Definition**

> Let the pmf (or pdf) of the random variable $X$ be $f_X(x)$. The expectation of $g(X)$ is 
$$\begin{aligned}
E[g(X)] &= \sum_{x:f_X(x)>0} g(x)f_X(x)~~~\mathrm{for~pmf} \\
        &= \int_{-\infty}^{\infty} g(x)f_X(x) dx~~~\mathrm{for~pdf}
\end{aligned}$$


#### **Properties**

> For any functions $g$ and $h$, any random variable $X$, and any constant $c$: 
> - $E(c) = c$
> - $E[cg(X)] = cE[g(X)]$
> - $E[g(X) + h(X)] = E[g(X)] + E[h(X)]$


---

# Expectation

#### **Relationship betwee n the expectation and sample mean**

<!-- > Question 1: Is a sample mean random variable?  -->
> - Suppose we have a random sample size of $N$, denoted as $\{X_1, X_2, \ldots, X_N\}$. Then 
sample mean of the random variable $X$ is 
> $$\bar{X} = \frac{1}{N}\sum_{i=1}^{N}X_i$$
> - Since $\{X_1, \ldots, X_N\}$ are randomly selected observations and each $X_i$ has the 
distribution, $\bar{X}$ is an random variable. 

$$\begin{aligned}
E(\bar{X}) &= E\left(\frac{1}{N}\sum_{i=1}^{N}X_i\right) \\
           &= \frac{1}{N}\sum_{i=1}^{N}E(X_i) = \frac{1}{N}N E(X) \\
           &= E(X) = \mu_X
\end{aligned}$$


---

# Variance 

#### **Variance of Random Variable**

> - The variance of a random variable indicates the shape of the pmf or pdf: whether the probability is 
dense or spread
> - Roughly speaking, the expectation is a predicted value from the random variable, while the variance 
indicates the precision or reliability of the prediction.


#### **Definition**

> - Let the pmf (or pdf) of a random variable $X$ be $f_X(x)$. The operator of the variance for a 
random variable $X$ is denoted as $\mathrm{Var}(X)$ and its result is written as $\sigma^2$

$$\begin{aligned}
\sigma^2 = \mathrm{Var}(X) &= E[(X - \mu)^2] \\
                           &= \sum_{x \in S}(x_i - \mu)^2f_X(x_i),~~~\mathrm{discrete} \\
                           &= \int_{-\infty}^{\infty} (x - \mu)^2f_X(x) dx,~~~\mathrm{continuous}
\end{aligned}$$


---
# Variance

#### **Properties**

> For any random variable $X$ and any constant $c$
> - $\mathrm{Var}(X) \geq 0$
> - $\mathrm{Var}(c) = 0$, $\mathrm{Var}(cX) = c^2\mathrm{Var}(X)$
> - $\mathrm{Var(X + c)} = \mathrm{Var(X)}$
> - $\mathrm{Var}(X) = E[X^2] - (E[X])^2 = E[X^2] - \mu_X^{2}$


#### The variance of sum (or difference) of two random variables

> Let $X$ and $Y$ be random variables with the probability distribution, then 

$$\begin{aligned}
\mathrm{Var}(X\pm Y) &= E[(X \pm Y - (\mu_X \pm \mu_Y))^2] \\
                     &= E[((X - \mu_X) \pm (Y - \mu_Y))^2] \\ 
                     &= E[(X - \mu_X)^2 + (Y - \mu_Y)^2 \pm 2(X-\mu_X)(Y - \mu_Y)] \\
                     &= E[(X - \mu_X)^2] + E[(Y - \mu_Y)^2] \pm 2E[(X-\mu_X)(Y - \mu_Y)] \\
                     &= \mathrm{Var}(X) + \mathrm{Var}(Y) \pm 2\mathrm{Cov}(X, Y)
\end{aligned}$$

---
# Variance 

#### `r fontawesome::fa("r-project", fill = "black")` Example

> Suppose $X \sim N(0, 1)$ and $Y \sim N(2, 4)$
> - Generate 100 random samples corresponding to $X$ and $Y$ 
> - Let $Z = X + Y$
> - Calcuate sample means and variances $X$, $Y$ and $Z$

.panelset[
.panel[.panel-name[R Code]
```{r, echo=TRUE}
set.seed(123)
X <- rnorm(100, 0, 1)
Y <- rnorm(100, 2, 2)
Z <- X+Y
mX <- mean(X); mY <- mean(Y)
mZ <- mean(Z)
vX <- var(X); vY <- var(Y)
vZ <- var(Z)

```
]

.panel[.panel-name[Results]
```{r, echo=TRUE, comment=NA}
mX + mY; mZ
vX + vY; vZ

```
]

]

---
# Variance 

#### **Variance of the Sample Mean**

> Let $X_1, \ldots, X_n$ are random samples from the identical and independent distribution, then 
> $$\mathrm{Var}(\bar{X}) = \frac{1}{N}\mathrm{Var}(X) = \frac{\sigma^2}{N}$$

#### **Proof**

$$\begin{aligned}
\mathrm{Var}(\bar{X}) &= \mathrm{Var}\left(\frac{1}{N}\sum_{i=1}^{N}X_i \right) = \frac{1}{N^2}\mathrm{Var(X_1 + \cdots + X_N)} \\ 
                      &= \frac{1} {N^2}N\mathrm{Var}(X) = \frac{1}{N}\mathrm{Var}(X) = \frac{\sigma^2}{N}
\end{aligned}$$

---
# Variance 

#### **Expectation of the Sample Variance** 
> Let $\bar{X}$, $\sigma^2_X$, and $S^2$ be the sample mean, variance, and sample variance of a random variable $X$. 
Then the expectation of $S^2$ is 
> $$E(S^2) = \frac{N-1}{N}\sigma^2_X,~~~S^2 = \frac{1}{N}\sum_{i=1}^{N}(X_i - \bar{X})^2$$


.panelset[
.panel[.panel-name[Proof: Part I]

$$\begin{aligned}
E(S^2) &= E\left[\frac{1}{N}\sum_{i=1}^{N}(X_i - \bar{X})^2 \right] = E\left[\frac{1}{N}\sum_{i=1}^{N}(X^2_i - 2X_i\bar{X} + \bar{X}^2) \right] \\ 
       &= E\left[\frac{1}{N}\sum_{i=1}^{N}X_i^2 -2\bar{X}\frac{1}{N}\sum_{i=1}^{N}X_i + \frac{1}{n}\sum_{i=1}^{N}\bar{X}^2  \right] \\ 
       &= \frac{1}{N}\sum_{i=1}^{N}E(X_i^2) - E(\bar{X}^2)
\end{aligned}$$

]

.panel[.panel-name[Part II]
Since $\sigma^2_X = E(X^2) - \mu_X^2$ and $\mathrm{Var}(\bar{X}) = E[\bar{X}^2] - \mu_X^2$

$$\begin{aligned}
E(X^2) &= \sigma^2_X + \mu_X^2 \\
E[\bar{X}^2] &= \frac{\sigma^2_X}{N} + \mu_X^2 
\end{aligned}$$
]

.panel[.panel-name[Part III]
Using Part II, 

$$\begin{aligned}
E(S^2) &= \frac{1}{N}\sum_{i=1}^{N}E(X_i^2) - E(\bar{X}^2) = \frac{1}{N}\sum_{i=1}^{N}(\sigma_X^2 + \mu_X^2) - E[\bar{X}^2] \\
       &= \frac{1}{N}N(\sigma^2_X + \mu_X^2) - \left(\frac{\sigma^2_X}{N} + \mu_X^2\right) \\ 
       & = \frac{N-1}{N}\sigma_X^2
\end{aligned}$$

]

.panel[.panel-name[Implication]

> - Unlike the sample mean $\bar{X}$, the expectation of the sample variance $S^2$ is less than $\sigma_X^2$ (**biased**)
> - To obtain the unbiased result, therefore, use $N-1$ 

]
]


---
# Expectation and Variance 

#### **Implication**

> 1. The p.m.f or p.d.f of $X$ shall exist to derive the expectation (or variance) but we don't know 
the exact formula of p.m.f or p.d.f. 
> 2. Since $E(\bar{X}) = E(X)$, the sample mean $\bar{x}$ is approximate to $E(X)$
> 3. If $N$ is very large, $\bar{x}$ is nearly or equal to $E(X)$ since the variance of $\bar{x}$ becomes small. 
> 4. Therefore, $\bar{x} \approx E(X)$ when $N$ is very large


???
$$\begin{aligned}
\sigma^2_X &= \frac{N}{N-1}E(S^2) \\
           &= \frac{N}{N-1}E\left[\frac{1}{N}\sum_{i=1}^{N}(X_i - \bar{X})^2\right] \\
           &= E\left[\frac{1}{N-1}\sum_{i=1}^{N}(X_i - \bar{X})^2\right]
\end{aligned}$$


---

# Joint and Marginal Distribution 

#### **Joint and Marginal Distribution for Two Categorical Vairables**

> Consider two discrete random variables $X$ and $Y$ with p.m.f $f_X$ and $f_Y$ supporting on the 
sample space $S_X$ and $S_Y$, respectively. Let $S_{X,Y}$ denote the set of all possible pairs 
$(x, y)$. Then the _**joint probability mass function**_ of $X$ and $Y$ is the function $f_{X,Y}$ is

$$f_{X,Y} = P(X = x, Y = y)~~~~\mathrm{for~(x,y) \in S_{X,Y}}$$

> For every joint p.m.f 

$$\begin{aligned}
f_{X, Y}(x, y) &> 0~~~~\mathrm{for~all~(x,y) \in S_{X,Y}} \\
\sum_{(x,y)\in S_{X,Y}} f_{X,Y}(x, y) &= \sum_{x\in S_X}\sum_{y \in S_Y} f_{X,Y}(x, y) = 1
\end{aligned}$$


---
# Joint and Marginal Distribution 

#### **Marginal Distribution**

> The p.m.f's of $f_{(X)}$ and $f_{(Y)}$ are called the _marginal p.m.f of $X$ and $Y$_, respectively, 
in the context of the joint probability distribution

$$\begin{aligned}
f_X(x) &= P(X = x) \\
       &= \sum_{y \in S_Y} P(X = x, Y = y) \\
       &= \sum_{y \in S_Y} f_{X,Y}(x, y)
\end{aligned}$$

> Similarly, 
> $$f_Y(y) = \sum_{x\in S_X} f_{X,Y}(x,y)$$


#### **Joint Cumulative Distribution Function**

$$F_{X,Y}(x,y)=P(X\leq x, Y \leq y)~~~~\mathrm{for~all~(x,y)}\in \mathbb{R}^2$$

???

Given the joint PMF we may recover the marginal PMFs, but the converse is not true. Even if
we have both marginal distributions they are not sufficient to determine the joint PMF; more
information is needed


**Cumulative probability mass function**

$$P(a\leq X\leq b, c \leq Y \leq d) = \sum_{a\leq X \leq b}\sum_{c\leq X \leq d} f_{X,Y}(x, y)$$
---
# Joint and Marginal Distribution 

#### **Example**

> The 50 students at a university take examinations in two subjects $X$ and $Y$, and results 
come out with A, B, C, D, E, F grades. 

.panelset[
.panel[.panel-name[R Code]
```{r, echo=TRUE}
library(tidyverse)

grades <- LETTERS[1:6]
dat <- expand_grid(X = grades, Y = grades)
dat <- dat %>% 
  mutate(n = c(1,2,1,0,0,0, 
               0,2,3,1,0,0, 
               0,4,7,4,1,0, 
               0,1,4,5,4,0,
               0,0,1,3,2,0, 
               0,0,0,1,2,1))
```

]
.panel[.panel-name[Table & Joint p.m.f]
.pull-left[
```{r, echo=TRUE, comment=NA}
library(questionr)
t1 <- with(dat, wtd.table(X, Y, 
           weights = n)); t1

```
]
.pull-right[
```{r, echo=TRUE, comment=NA}
# Joint probability mass function
t1/sum(t1)
```
]
]

.panel[.panel-name[Heatmap-Code]
```{r, echo=TRUE}
library(ggpubr)
gp1 <- ggplot(data = dat) + 
  aes(x = X, y = Y, fill = n/60, label = sprintf("%.2f", n/60)) + 
  geom_tile(color = "white") + 
  scale_fill_gradient(
    low = "white", high = "#132B43", limits = c(0, 0.15), 
    name = "", breaks = c(0, 0.05, 0.10, 0.15)
  ) +
  labs(title = "Joint Probabilty Mass Function of X and Y") + 
  geom_text(color = "black", size = 4) +
  guides(fill = guide_colorbar(barheight = 25)) + 
  scale_y_discrete(limits = rev) + 
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(), axis.title = element_blank())

```

]

.panel[.panel-name[Plot]
```{r pmf-ex, fig.width=8, fig.height=8, out.height=380, out.width=380}
gp1
```

]

.panel[.panel-name[Marginal Distribution]
.pull-left[
```{r, echo=TRUE, comment=NA}
# Marginal pmf of X
dat %>% 
  count(X, wt = n) %>% 
  mutate(n = n/sum(n))
```

]
.pull-right[
```{r, echo=TRUE, comment=NA}
# Marginal pmf of YY
dat %>% 
  count(Y, wt = n) %>% 
  mutate(n = n/sum(n))
```

]
]

]


---
# Joint and Marginal Distribution

#### **Joint and Marginal Distribution for Continuous Variables**

> Given two continuous random variable $X$ and $Y$, let $f_{X,Y}(x, y)$ be a function associated 
with $X = x$ and $Y = y$. Then $f$ is called the _**joint probability density function**_ of 
$X$ and $Y$ if 

$$\begin{aligned}
f_{X,Y}(x,y) &\geq 0~~~~\mathrm{for~all}~(x,y)\in S_{X,Y} \\
\int_{S_X}\int_{S_Y} f_{X,Y}(x,y)dx dy &= 1
\end{aligned}$$


> Joint Cumulative Distribution Function 

$$F_{X,Y}(x, y) = P(X \leq x, Y\leq y) = \int_{-\infty}^{x}\int_{-\infty}^{y} f_{X,Y}(u, v) dv du
~~~~\mathrm{for~all}~(x,y) \in \mathbb{R}^2$$

> Marginal PDF of $X$ and $Y$

$$\begin{aligned}
f_X(x) &= \int_{S_Y} f_{X,Y}(x,y) dy,~~~~x \in S_X \\
f_Y(y) &= \int_{S_X} f_{X,Y}(x,y) dx,~~~~y \in S_Y \\
\end{aligned}$$



???
(**For Joint CDF**) In the continuous case there is not such a simple interpretation for the joint 
PDF; however,we do have one for the joint CDF, namely


---
# Joint and Marginal Distribution

#### **Example**: Joint PDF

> Let the joint PDF of $(X,Y)$ be given by 
> $$f_{X,Y}(x,y) = \frac{6}{5}(x + y^2), ~~~~0<x<1,~0<y<1$$

.panelset[
.panel[.panel-name[Marginal PDF of X]
$$\begin{aligned}
f_X(x) &= \int_0^1 \frac{6}{5}(x + y^2) dy \\
       &= \frac{6}{5}\left(xy + \frac{y^3}{3}\right) \bigg|_{y=0}^{1} \\
       &= \frac{6}{5}\left(x + \frac{1}{3}\right)
\end{aligned}$$
]
.panel[.panel-name[Marginal PDF of Y]
$$\begin{aligned}
f_X(y) &= \int_0^1 \frac{6}{5}(x + y^2) dx \\
       &= \frac{6}{5}\left(\frac{x^2}{2} + xy^2\right) \bigg|_{x=0}^{1} \\
       &= \frac{6}{5}\left(\frac{1}{2} + y^2\right)
\end{aligned}$$
]
.panel[.panel-name[Probabilty of joint PDF]
$$\begin{aligned}
P(0<x<0.3,~0.1<y<0.3) &= \int_{x=0}^{0.3}\int_{y=0.1}^{0.3}\frac{6}{5}\left(x + y^2\right) dy dx \\
                      &= \int_{x=0}^{0.3}\frac{6}{5}\left(xy + \frac{y^3}{3}\right)\bigg|_{y=0.1}^{0.3} dx \\
                      &= \int_{x=0}^{0.3}\frac{6}{5}\left(0.2x + 0.00867\right) dx \\
                      &= \frac{6}{5}\left(\frac{0.2}{2}x^2 + 0.00867x\right)\bigg |_{x=0}^{0.3} \approx 0.014
\end{aligned}$$
]

]

---
# Joint and Marginal Expectation

> Given a function $g$ with arguments $(x, y)$, The expectation of $g(x, y)$ with respect to the 
joint PDF (or PMF) is

$$\begin{aligned}
E\left[g(X,Y)\right] &= \int_{S_X}\int_{S_Y}g(x,y)f_{X,Y}(x,y) dx dy,~~~~\mathrm{for~the~continuous~case} \\
E\left[g(X,Y)\right] &= \sum_{x\in S_X}\sum_{y \in S_Y}g(x,y)f_{X,Y}(x,y),~~~~\mathrm{for~the~discrete~case}
\end{aligned}$$

#### **Example**

> From the previous example for the joint PDF of $X$ and $Y$, 

$$\begin{aligned}
E[X]  &= \int_{x=0}^{1}\int_{y=0}^{1} x\frac{6}{5}(x + y^2) dy dx = 0.6 \\
E[XY] &= \int_{x=0}^{1}\int_{y=0}^{1} xy\frac{6}{5}(x + y^2) dy dx = 0.35 \\
E[X^2] &= \int_{x=0}^{1}\int_{y=0}^{1} x^2\frac{6}{5}(x + y^2) dy dx = 0.43
\end{aligned}$$

---
# Joint and Marginal Expectation

#### **Covariance and Correlation**

> Two special cases of joint expectations $\rightarrow$ quantifying the dependence (relationship) between $X$ and $Y$

#### **Definition: Covariance**

> The _**covariance**_ of $X$ and $Y$ is defined as 

$$\begin{aligned}
\mathrm{Cov}(X,Y) = \sigma_{XY}&= E[X - E(X)]E[Y - E(Y)] \\
                  &= E[XY] - E[X]E[Y]
\end{aligned}$$


#### **Definition: Correlation**

> The Pearson product moment correlation between $X$ and $Y$ is the covariance between $X$ and
$Y$ rescaled to fall in the interval [−1, 1]


$$\begin{aligned}
\mathrm{Corr}(X,Y) = \rho_{XY} &= E\left[\left(\frac{X - \mu_X}{\sigma_X}\right)\left(\frac{X-\mu_Y}{\sigma_Y}\right)\right] \\
                               &= \frac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}
\end{aligned}$$

---
# Joint and Marginal Expectation

#### **Homework for Practice**

> Let the joint PMF of $X$ and $Y$ be given as

```{r}
pdf <- c(0.05, 0.05, 0.1, 0, 
         0.05, 0.1, 0.25, 0.1, 
         0, 0.15, 0.1, 0.05)
tab <- matrix(pdf, byrow = TRUE, nrow = 3)
rownames(tab) = paste0("X=", 0:2)
colnames(tab) = paste0("Y=", 0:3)

kbl(tab) %>% kable_paper
  

```


> - Define the marginal PMF of $X$ and $Y$
> - Obtain $E(X)$, $E(Y)$, $Var(X)$, $Var(Y)$ for the marginal distribution of $X$ and $Y$
> - Obtain $E(XY)$
> - Obtain $\mathrm{Cov}(X,Y)$
> - Obtain $\mathrm{Corr}(X,Y)$





